# Clustering Algorithms Comparison Project 
<img width="400" height="280" alt="image" src="https://github.com/user-attachments/assets/68a3a57a-a594-44f1-931b-c2f4256663b8" />
  
A comprehensive Python project that implements and compares the performance of various clustering algorithms on synthetic datasets generated using `sklearn.datasets.make_blobs`.
  
## Overview
This project demonstrates the application of fundamental and advanced clustering techniques to identify groups and patterns within multidimensional synthetic data. It serves as an excellent educational resource for understanding how different clustering algorithms behave under controlled conditions.

## Algorithms Implemented 
This project provides a comprehensive comparison of both fundamental and advanced clustering techniques:

#### Centroid-Based Methods

- **K-Means**: A classic centroid-based algorithm that partitions data into K distinct, spherical clusters by minimizing within-cluster variance.

#### Density-Based Methods

- **DBSCAN** (Density-Based Spatial Clustering of Applications with Noise): Identifies arbitrary shaped clusters and detects outliers based on spatial density connectivity.

- **OPTICS** (Ordering Points To Identify the Clustering Structure): An advanced density-based method that creates a reachability plot for variable density cluster analysis.

#### Hierarchical Methods

- **Agglomerative Hierarchical Clustering**: Builds a hierarchy of clusters using a bottom-up approach, merging similar clusters at each step.

#### Distribution-Based Methods

- **Affinity Propagation**: Creates clusters by passing messages between data points to identify exemplars that best represent cluster centers.

#### Advanced Techniques

- **Mean Shift Clustering** : A non-parametric technique that locates and adapts centroids based on kernel density estimation.

- **Spectral Clustering**: Utilizes graph theory and eigenvalues of similarity matrices to perform dimensionality reduction before clustering.

## Tech Stack

- Libraries:
  
  - Scikit-learn (for algorithms and dataset generation)

  - Matplotlib, Seaborn (for data visualization)

  - Numpy, Pandas (for data manipulation)

- Dataset: Synthetic data generated via `sklearn.datasets.make_blobs`


## Results and Analysis
The comparative analysis reveals distinct performance characteristics for each algorithm on the synthetic blob dataset:

#### Outstanding Performance of K-Means
The K-Means algorithm demonstrated exceptional performance on this dataset, achieving near-perfect clustering results. This was expected due to the inherent properties of our synthetic data:

- High Accuracy: K-Means correctly identified all three distinct spherical clusters with minimal misclassification.

- Optimal Cluster Separation: The algorithm perfectly captured the well-separated, isotropic Gaussian distributions generated by make_blobs.

- Clear Centroids: The calculated centroids aligned almost perfectly with the true cluster centers at [(-5, -5), (0, 0), (5, 5)].

- Visualization: The K-Means clustering plot shows three distinct, well-separated clusters with centroids positioned at the core of each blob, demonstrating the algorithm's effectiveness for spherical, evenly-distributed data.

##### Comparative Insights

<img width="1585" height="525" alt="newplot (31)" src="https://github.com/user-attachments/assets/cf2a19ab-6d90-4157-bf2b-928fe72b88ff" />


While K-Means excelled, other algorithms showed varying results:

- DBSCAN and OPTICS effectively identified clusters but struggled with parameter tuning for this specific configuration

- Agglomerative Clustering produced clean results but required pre-specification of the number of clusters

- Spectral Clustering performed well but with higher computational complexity

- Affinity Propagation and Mean Shift demonstrated adaptive cluster detection without requiring pre-defined cluster numbers

This makes K-Means an excellent baseline algorithm for similar clustering tasks and explains its superior performance in this specific scenario.
